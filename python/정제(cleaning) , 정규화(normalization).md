# 정제(cleaning) , 정규화(normalization)

토큰화 작업 전후에서 함께 이루어지며 텍스트 용도에 맞게 정제, 정규화를 함

- 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다
- 정규화(normalization) : 표현방법이 다른 단어들을 같은 단어로 통합시켜줌

정제 작업은 토큰화에 방해되는 것들을 제거하는 데 의미가있는데 완벽히 하기 어려워

합의점을 가질수 있음

## 1. 규칙에 기반한 표기가 다른 단어들의 통합

필요에 따라 직접 코딩을 통해 정의할 수 있는 정규화

아래는 표기가 다른 단어들을 통합하는 방법

- 어간 추출(stemming)
- 표제어 추출(lemmatization) 

## 2. 대, 소문자 통합

영어권에서는 대,소문자를 구분하는 것 자체로도 단어의 개수를 줄이는 또 다른 정규화 방법이다.

하지만 대,소문자 통합을 무작정 할 수 있는 건 아니다, 대문자이여야만 뜻이 통하는 USA 등,

회사이름 (Genenal Motors) 등 이런 것들이 있을 수 있다.

- 머신러닝 - 시퀀스 모델 : 소문자 변환을 언제 사용할지 결정하는 모델이 있다.

하지만 이 모델은 데이터가 대,소문자 구분이 올바른 방법으로 확실히 되어있어야 된다.

## 3. 불필요한 단어의 제거

정제 작업에서 제거해야하는 노이즈 데이터는 자연어가 아니면서 아무 의미도 갖지 않는 글자들

을 의미하긴 하지만, 분석에 맞지 않은 불필요 단어들도 노이즈 데이터라 할 수 있다.

### 1) 등장 빈도가 적은 단어

말 그대로 데이터에서 너무, 엄청 적게 등장하는 단어들을 의미함, ex) 100,000개의 메일에서 5개의  메일만 나오는 경우 5개의 의미는 없을 것이다.

### 2) 길이가 짧은 단어 

영어권에서는 실용성있는 제거 방법이다. it, if , a , to , on , in 등 짧은 단어들이 많아 제거에 의미가

있다. 하지만 한국어는 영어에 비해 단어들이 영어에 비해 짧고 함축적인 의미가 많아 실용성이 없을 

수 있다.

```python
# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제
import re
text = "I was wondering if anyone out there could enlighten me on this car."
shortword = re.compile(r'\W*\b\w{1,2}\b')
print(shortword.sub('', text))
```

## 4. 정규 표현식 (regular expression)

얻어낸 코퍼스에서 노이즈 데이터의 패턴을 알 수 있으면 정규 표현식으로 이를 제거할 수 있는 경우

정규표현식을 사용한다. 

ex) 뉴스를 크롤링 했다면 기자이름, 게재 시간 등 목적에 필요없으며 패턴이 존재하는 것